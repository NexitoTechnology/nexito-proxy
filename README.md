# Proxy Service

<div align="center">

![Proxy Service Logo](https://via.placeholder.com/150?text=Proxy+Service)

**Sistema de gesti√≥n de proxies para scraping web a gran escala**

[![Python 3.10+](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.88.0+-green.svg)](https://fastapi.tiangolo.com/)
[![MongoDB](https://img.shields.io/badge/MongoDB-4.0+-green.svg)](https://www.mongodb.com/)

</div>

## üìã Resumen

Proxy Service es un sistema avanzado que automatiza la obtenci√≥n, validaci√≥n y gesti√≥n de proxies para operaciones de scraping web a gran escala. Dise√±ado como un servicio independiente con una API REST, proporciona proxies de alta calidad y mantiene estad√≠sticas detalladas de rendimiento.

## ‚ú® Caracter√≠sticas Principales

- üîç **Scraping Autom√°tico**: Obtiene proxies de m√∫ltiples fuentes p√∫blicas
- üß™ **Validaci√≥n Inteligente**: Verifica el funcionamiento de los proxies contra Google
- üèÜ **Sistema de Puntuaci√≥n**: Califica los proxies seg√∫n velocidad, estabilidad y funcionalidad
- üîÑ **Rotaci√≥n Din√°mica**: Proporciona proxies frescos para evitar bloqueos
- üìä **Monitoreo de Rendimiento**: Mantiene estad√≠sticas detalladas de cada proxy
- ‚è±Ô∏è **Actualizaci√≥n Peri√≥dica**: Refresca autom√°ticamente la base de datos de proxies
- üîå **API REST Completa**: F√°cil integraci√≥n con sistemas de scraping

## üöÄ Inicio R√°pido

### Requisitos

- Python 3.10+
- MongoDB 4.0+
- Docker (opcional, para MongoDB)

### Instalaci√≥n

1. **Clonar el repositorio**

```bash
git clone https://github.com/tu-usuario/proxy-service.git
cd proxy-service
```

2. **Configurar entorno virtual**

```bash
# Crear entorno virtual
python -m venv venv

# Activar entorno virtual
# En Windows:
venv\Scripts\activate
# En macOS/Linux:
source venv/bin/activate
```

3. **Instalar dependencias**

```bash
pip install -r requirements.txt
```

4. **Configurar MongoDB**

```bash
# Opci√≥n con Docker
docker run -d -p 27017:27017 --name mongodb mongo:latest
```

5. **Configurar variables de entorno**

Crea un archivo `.env` en la ra√≠z del proyecto con el siguiente contenido:

```
# API Settings
API_KEY=your_secret_api_key_here

# MongoDB Settings
MONGODB_URL=mongodb://localhost:27017
MONGODB_DB=proxy_service
```

### Ejecuci√≥n

```bash
uvicorn app.main:app --reload
```

El servicio estar√° disponible en: http://127.0.0.1:8000

Documentaci√≥n Swagger UI: http://127.0.0.1:8000/docs

## üîå Uso de la API

### Autenticaci√≥n

Todos los endpoints requieren autenticaci√≥n mediante la cabecera `x-api-key`.

```bash
curl -X GET "http://localhost:8000/api/proxies" -H "x-api-key: your_secret_api_key_here"
```

### Endpoints Principales

#### Gesti√≥n de Proxies

| M√©todo | Endpoint | Descripci√≥n |
|--------|----------|-------------|
| GET | `/api/proxy` | Obtener un √∫nico proxy v√°lido |
| GET | `/api/proxies` | Obtener m√∫ltiples proxies filtrados |
| POST | `/api/proxy` | A√±adir un nuevo proxy manualmente |
| POST | `/api/proxy/report` | Reportar el resultado de usar un proxy |

#### Scraping de Proxies

| M√©todo | Endpoint | Descripci√≥n |
|--------|----------|-------------|
| POST | `/api/scrape/all` | Obtener proxies de todas las fuentes registradas |
| POST | `/api/scrape/{source_name}` | Obtener proxies de una fuente espec√≠fica |

#### Validaci√≥n de Proxies

| M√©todo | Endpoint | Descripci√≥n |
|--------|----------|-------------|
| POST | `/api/validate/all` | Validar todos los proxies en la base de datos |
| POST | `/api/validate/{ip}/{port}` | Validar un proxy espec√≠fico |

### Ejemplos de Uso

#### Obtener un Proxy V√°lido

```bash
curl -X GET "http://localhost:8000/api/proxy" \
     -H "x-api-key: your_secret_api_key_here" \
     -H "accept: application/json"
```

Respuesta:

```json
{
  "ip": "203.0.113.1",
  "port": 8080,
  "protocol": "http",
  "country": "United States",
  "anonymity": "high",
  "status": "active",
  "score": 92,
  "last_checked": "2025-05-11T10:23:45.123456",
  "source": "free_proxy_list"
}
```

#### Iniciar Scraping de Proxies

```bash
curl -X POST "http://localhost:8000/api/scrape/all" \
     -H "x-api-key: your_secret_api_key_here" \
     -H "accept: application/json"
```

Respuesta:

```json
42
```

(N√∫mero de proxies a√±adidos a la base de datos)

## üèóÔ∏è Arquitectura

### Estructura del Proyecto

```
proxy-service/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ api/            # Endpoints y routers de la API
‚îÇ   ‚îú‚îÄ‚îÄ core/           # Configuraci√≥n y componentes centrales
‚îÇ   ‚îú‚îÄ‚îÄ db/             # Conexi√≥n y operaciones con la base de datos
‚îÇ   ‚îú‚îÄ‚îÄ models/         # Modelos de datos y esquemas
‚îÇ   ‚îú‚îÄ‚îÄ scrapers/       # M√≥dulos para scraping de diferentes fuentes
‚îÇ   ‚îú‚îÄ‚îÄ services/       # L√≥gica de negocio
‚îÇ   ‚îú‚îÄ‚îÄ validators/     # Validaci√≥n de proxies
‚îÇ   ‚îî‚îÄ‚îÄ main.py         # Punto de entrada de la aplicaci√≥n
‚îú‚îÄ‚îÄ logs/               # Logs del servicio
‚îú‚îÄ‚îÄ .env                # Variables de entorno
‚îú‚îÄ‚îÄ requirements.txt    # Dependencias
‚îî‚îÄ‚îÄ README.md           # Este archivo
```

### Componentes Principales

- **API Layer**: Gestiona las solicitudes HTTP y devuelve respuestas
- **Services Layer**: Implementa la l√≥gica de negocio
- **Data Layer**: Gestiona el almacenamiento y recuperaci√≥n de datos
- **Scrapers**: Obtienen proxies de diferentes fuentes
- **Validators**: Verifican el funcionamiento de los proxies
- **Scheduler**: Ejecuta tareas peri√≥dicas de scraping y validaci√≥n

### Flujo de Datos

1. El scheduler activa peri√≥dicamente el scraping de proxies
2. Los scrapers obtienen proxies de diferentes fuentes
3. Los proxies son almacenados en la base de datos
4. El scheduler activa la validaci√≥n peri√≥dica
5. El validator verifica cada proxy y actualiza su estado
6. La API proporciona acceso a los proxies validados
7. Los clientes reportan el rendimiento de los proxies utilizados

## üß© Extensibilidad

### A√±adir un Nuevo Scraper

1. Crear un nuevo archivo en `app/scrapers/`, por ejemplo `my_source_scraper.py`

```python
from typing import List
from bs4 import BeautifulSoup
from .base_scraper import BaseScraper
from ..models.proxy import Proxy, ProxyProtocol, ProxyStatus

class MySourceScraper(BaseScraper):
    """Scraper for my-source.com"""
    
    name = "my_source"
    url = "https://my-source.com/proxies"
    
    async def scrape(self) -> List[Proxy]:
        # Implementaci√≥n espec√≠fica
        proxies = []
        
        # A√±adir l√≥gica de scraping aqu√≠
        
        return proxies
```

2. Registrar el scraper en `app/services/scraper_service.py`

```python
# Importar el nuevo scraper
from ..scrapers.my_source_scraper import MySourceScraper

class ScraperService:
    _scrapers: Dict[str, Type[BaseScraper]] = {
        "free_proxy_list": FreeProxyListScraper,
        "my_source": MySourceScraper  # A√±adir aqu√≠
    }
    # ...
```

## üìà Rendimiento y Escalabilidad

- **Procesamiento As√≠ncrono**: Utiliza asyncio para operaciones concurrentes
- **Procesamiento por Lotes**: Valida proxies en lotes para optimizar recursos
- **√çndices Optimizados**: Estructura de MongoDB optimizada para consultas frecuentes
- **Cach√© Inteligente**: Minimiza consultas a la base de datos
- **Operaci√≥n Distribuida**: Puede ejecutarse en m√∫ltiples instancias

## üîß Configuraci√≥n Avanzada

### Variables de Entorno

| Variable | Descripci√≥n | Valor por defecto |
|----------|-------------|-------------------|
| `API_KEY` | Clave de autenticaci√≥n para la API | `your_secret_api_key_here` |
| `MONGODB_URL` | URL de conexi√≥n a MongoDB | `mongodb://localhost:27017` |
| `MONGODB_DB` | Nombre de la base de datos | `proxy_service` |
| `PROXY_VALIDATION_INTERVAL` | Intervalo de validaci√≥n (segundos) | `3600` |
| `PROXY_MIN_SCORE` | Puntuaci√≥n m√≠nima para considerar un proxy v√°lido | `50` |
| `SCRAPING_INTERVAL` | Intervalo de scraping (segundos) | `21600` |

### Configuraci√≥n de Logging

El servicio utiliza Loguru para logging. Los logs se guardan en `logs/proxy_service.log` y rotan autom√°ticamente cuando alcanzan 10 MB.

## ü§ù Integraci√≥n con Otros Sistemas

Este servicio est√° dise√±ado para integrarse con sistemas de scraping web. El cliente puede solicitar proxies a trav√©s de la API, utilizarlos para sus operaciones de scraping, y luego reportar su rendimiento.

### Ejemplo de Integraci√≥n con Python

```python
import httpx

class ProxyClient:
    def __init__(self, api_url, api_key):
        self.api_url = api_url
        self.api_key = api_key
        self.headers = {"x-api-key": api_key}
    
    async def get_proxy(self):
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"{self.api_url}/api/proxy",
                headers=self.headers
            )
            return response.json()
    
    async def report_result(self, ip, port, success, latency_ms=None, error=None):
        async with httpx.AsyncClient() as client:
            await client.post(
                f"{self.api_url}/api/proxy/report",
                headers=self.headers,
                params={
                    "ip": ip,
                    "port": port,
                    "success": success,
                    "latency_ms": latency_ms,
                    "error": error
                }
            )
```

## üìã Licencia

Este proyecto est√° licenciado bajo los t√©rminos de la licencia MIT.

---

<div align="center">
Desarrollado con ‚ù§Ô∏è por [Tu Nombre/Organizaci√≥n]
</div>